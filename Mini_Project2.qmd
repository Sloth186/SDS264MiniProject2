---
title: "Mini-Project2"
author: "Tenzin Gyaltsen, Shen Rothermel"
format:
  pdf: default
editor_options: 
  chunk_output_type: console
---

## Introduction

- blah, blah, yap, yap. 

## Motivation

Include:

- What kind of data you're scraping
- Why you're interested in it
- Example questions you'd like to answer

## Scraping the “Squad Standard Stats” table:

```{r}
library(rvest)
library(dplyr)
library(janitor)
library(stringr)

# Check permissions for the specific stats page
robotstxt::paths_allowed("https://fbref.com/en/comps/22/Major-League-Soccer-Stats")

# Step 1: Read the page with rvest
MLS_table <- read_html("https://fbref.com/en/comps/22/Major-League-Soccer-Stats")

# Step 2: Extract tables from the page
Squad <- html_nodes(MLS_table, "table")
html_table(Squad, header = TRUE, fill = TRUE)  # find right table

# Step 3: Extract the correct table (the fifth table on the page)
Squad2 <- html_table(Squad, header = TRUE, fill = TRUE)[[5]]
Squad2

# Step 4: Keep only relevant columns and clean the data
Squad2_cleaned <- Squad2 |>
  row_to_names(row_number = 1) |>   # promotes row 1 to column names
  clean_names() |>                  # make the column names snake_case
  select(1:16) |>                   # keep only the first 16 columns
  filter(squad != "Squad") |>       # remove header repeats if any
  mutate(across(2:16, parse_number))# apply parse_number to cols 2–16
```

```{r}
# Custom Function
scrape_fbref_table <- function(url, table_number = 5, n_cols = 16) {
  page <- read_html(url)
  tables <- html_nodes(page, "table")
  raw_table <- html_table(tables, fill = TRUE)[[table_number]]
  
  cleaned_table <- raw_table |>
    row_to_names(row_number = 1) |>
    clean_names() |>
    select(1:n_cols) |>
    filter(squad != "Squad") |>
    mutate(across(2:n_cols, parse_number))
  
  return(cleaned_table)
}

Squad2_cleaned <- scrape_fbref_table("https://fbref.com/en/comps/22/Major-League-Soccer-Stats")
```

